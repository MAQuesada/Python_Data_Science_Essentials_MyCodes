{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with library **NLTK**, the Natural Language Tool Kit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization: \n",
    "Es el acto de dividir el texto en palabras. En el modulo nltk.tokenize hay varios tokenizations por ejemplo TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'coolest', 'job', 'in', 'the', 'next', '10', 'years', 'will', 'be', 'statisticians', '.', 'People', 'think', 'I', \"'m\", 'joking', ',', 'but', 'who', 'would', \"'ve\", 'guessed', 'that', 'computer', 'engineers', 'would', \"'ve\", 'been', 'the', 'coolest', 'job', 'of', 'the', '1990s', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "my_text = \"The coolest job in the next 10 years will be \" +\\\n",
    "\"statisticians . People think I'm joking, but \" +\\\n",
    "\"who would've guessed that computer engineers \" +\\\n",
    "\"would've been the coolest job of the 1990s?\"\n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(my_text,preserve_line=True)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "El stemming es la acción de reducir las formas flexivas de las palabras y llevarlas a sus conceptos básicos. Por ejemplo, el concepto de *is ,be , are y am* es el mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'coolest', 'job', 'in', 'the', 'next', '10', 'year', 'wil', 'be', 'stat', '.', 'peopl', 'think', 'i', \"'m\", 'jok', ',', 'but', 'who', 'would', \"'ve\", 'guess', 'that', 'comput', 'engin', 'would', \"'ve\", 'been', 'the', 'coolest', 'job', 'of', 'the', '1990s', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import *\n",
    "stemmer = LancasterStemmer()\n",
    "l = [stemmer.stem(word) for word in nltk_tokens]\n",
    "\n",
    "print(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo, hemos utilizado el stemmer de Lancaster, que es uno de\n",
    "los algoritmos más potentes y recientes. Al comprobar el resultado, verá\n",
    "inmediatamente que está todo en minúsculas y la palabra *statisticians* está\n",
    "asociado a su raíz: *stat* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tagging\n",
    "Tagging, o POS-Tagging, es la asociación entre una palabra (o un\n",
    "token) y su etiqueta de parte de oración (POS-Tag). Después del\n",
    "etiquetado, usted sabe qué (y dónde) están los verbos, adjetivos,\n",
    "sustantivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.pos_tag(nltk.word_tokenize(\"John's big idea isn't all that bad.\",\n",
    "             preserve_line=True), tagset='universal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords(Palabras claves)\n",
    "Las stopwords son los tokens menos informativos del texto, ya\n",
    "que son las palabras más comunes (como the, it, is, as y not). Las\n",
    "stopwords suelen eliminarse y asi el procesamiento requiere menos tiempo\n",
    "y menos memoria; además, a veces es más preciso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'down', 'via', 'herself', 'thereafter', 'indeed', 'nobody', 'already', 'sincere', 'whither', 'either', 'cannot', 'everything', 'on', 'again', 'so', 'a', 'of', 'above', 'top', 'without', 'further', 'nor', 'and', 'are', 'all', 'one', 'latterly', 'whenever', 'also', 'seem', 'she', 'seemed', 'eight', 'when', 'anywhere', 'only', 'nowhere', 'under', 'how', 'another', 'everywhere', 'first', 'therein', 'why', 'not', 'hundred', 'is', 'hers', 'whence', 'that', 'part', 'it', 'eleven', 'detail', 'forty', 'amount', 'least', 'ten', 'then', 'cry', 'noone', 'although', 'whoever', 'bottom', 'up', 'along', 'was', 'neither', 'three', 'afterwards', 'from', 'whatever', 'around', 'both', 'am', 'now', 'our', 'his', 'anyhow', 'see', 'many', 'whereafter', 'onto', 'empty', 'ie', 'about', 'meanwhile', 'them', 'often', 'few', 'has', 'becoming', 'name', 'besides', 'sixty', 'whose', 'sometime', 'nevertheless', 'most', 'except', 'fifty', 'fire', 'back', 'before', 'own', 'whole', 'in', 'could', 'at', 'him', 'myself', 'have', 'yourself', 'they', 'the', 'call', 'cant', 'elsewhere', 'as', 'for', 'throughout', 'un', 'perhaps', 'whether', 'enough', 'been', 'across', 'hereafter', 'after', 'else', 'go', 'whom', 'we', 'to', 'show', 'twenty', 'every', 'out', 'eg', 'two', 'what', 'beside', 'any', 'whereby', 'co', 'mostly', 'anyone', 'thence', 'should', 'will', 'because', 'fill', 'their', 'everyone', 'hence', 'if', 'due', 'several', 'seems', 'being', 'where', 'de', 'me', 'might', 'other', 'amongst', 'more', 'per', 'once', 'us', 'none', 'themselves', 'its', 'formerly', 'yours', 'mill', 'still', 'take', 'wherein', 'toward', 'some', 'itself', 'ours', 'never', 'nine', 'do', 'thin', 'get', 'herein', 'beyond', 'anything', 'alone', 'may', 'couldnt', 'an', 'less', 'con', 'something', 'there', 'must', 'keep', 'sometimes', 'too', 'therefore', 'next', 'would', 'here', 'while', 'you', 'though', 'or', 'five', 'becomes', 'put', 'between', 'off', 'someone', 'he', 'during', 'thru', 'whereupon', 'against', 'somehow', 'were', 'front', 'with', 'always', 'please', 'no', 'yet', 'wherever', 'very', 'thereupon', 'nothing', 'side', 'upon', 'four', 'inc', 'mine', 'moreover', 'others', 'beforehand', 'by', 'i', 'made', 'much', 'ourselves', 'behind', 'such', 'yourselves', 'six', 'found', 'hasnt', 'rather', 'became', 'each', 'himself', 'serious', 'since', 'last', 'this', 'than', 'ltd', 'who', 'otherwise', 'same', 'give', 'ever', 'fifteen', 'below', 'however', 'together', 'twelve', 'but', 'had', 'those', 'somewhere', 'move', 'these', 'within', 'well', 'become', 'towards', 'over', 'namely', 're', 'which', 'my', 'bill', 'hereby', 'through', 'third', 'even', 'hereupon', 'whereas', 'describe', 'system', 'interest', 'until', 'into', 'thus', 'amoungst', 'full', 'thereby', 'her', 'find', 'former', 'latter', 'your', 'thick', 'almost', 'be', 'among', 'anyway', 'seeming', 'etc', 'can', 'done'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A complete data science example –text classification** (otros ejemplos estan en el archivo *2_data_munging.ipynb*)\n",
    "Un ejemplo con el conjunto de datos 20newsgroup, que ya se introdujo en el capítulo1. Para\n",
    "hacer las cosas más realistas y evitar que el clasificador sobreajuste los\n",
    "datos, eliminaremos las cabeceras de los correos electrónicos, los pies de\n",
    "página (como una firma) y las comillas. Además, en este caso, el objetivo es\n",
    "clasificar entre dos categorías similares: *sci.med* y *sci.space*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "categories = ['sci.med', 'sci.space']\n",
    "to_remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "twenty_sci_news_train = fetch_20newsgroups(subset='train',\n",
    "                                           remove=to_remove, categories=categories)\n",
    "twenty_sci_news_test = fetch_20newsgroups(subset='test',\n",
    "                                          remove=to_remove, categories=categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos **Tfidf** que  es la multiplicación de  frecuencia de la palabra en el documento por la inversa de \n",
    "su frecuencia en todos los documentos. Las puntuaciones altas indican que la palabra se\n",
    "utiliza varias veces en el documento actual, pero es poco frecuente en los demás\n",
    "(es decir, es una palabra clave del documento):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"you\" -- 0.167       \"want\" -- 0.303       \"to\" -- 0.109       \"the\" -- 0.101       \"star\" -- 0.421       \"sounds\" -- 0.401       \"plane\" -- 0.474       \"me\" -- 0.233       \"like\" -- 0.219       \"ground\" -- 0.415       \"for\" -- 0.146      "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\nSounds to me like you'd want a star for the ground plane.\\n\""
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vect = TfidfVectorizer()\n",
    "X_train = tf_vect.fit_transform(twenty_sci_news_train.data)\n",
    "X_test = tf_vect.transform(twenty_sci_news_test.data)\n",
    "y_train = twenty_sci_news_train.target\n",
    "y_test = twenty_sci_news_test.target\n",
    "\n",
    "word_list = tf_vect.get_feature_names_out()\n",
    "for n in X_test[0].indices:\n",
    "    print(' \"%s\" -- %0.3f' %  (word_list[n], X_test[0, n]), end='      ')\n",
    "\n",
    "twenty_sci_news_test.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''como la norma por defecto es 'l2' la suma de los cuadrados de cada fila es igual a uno'''\n",
    "import numpy as np\n",
    "\n",
    "np.power(X_test[0].todense(), 2).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8911392405063291\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy=\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver si usuando las tecnicas de limpieza de texto aprendidas en este capitulo podemos mejorar la **Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "\n",
    "def clean_and_stem_text(text):\n",
    "    \"\"\"t básicamente pone en minúsculas, tokeniza,stems y reconstruye cada \n",
    "    documento del conjunto de datos\"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower(),preserve_line=True)\n",
    "    clean_tokens = [word for word in tokens if word not in stop_words]\n",
    "    stem_tokens = [stemmer.stem(token) for token in clean_tokens]\n",
    "    return \" \".join(stem_tokens)\n",
    "\n",
    "\n",
    "cleaned_docs_train = [clean_and_stem_text(text) for text in twenty_sci_news_train.data]\n",
    "cleaned_docs_test = [clean_and_stem_text(text) for text in twenty_sci_news_test.data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8924050632911392\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X1_train = tf_vect.fit_transform(cleaned_docs_train)\n",
    "X1_test = tf_vect.transform(cleaned_docs_test)\n",
    "clf.fit(X1_train, y_train)\n",
    "y1_pred = clf.predict(X1_test)\n",
    "print(\"Accuracy=\", accuracy_score(y_test, y1_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este proceso requiere más tiempo, pero conseguimos una precisión mejor. Un ajuste preciso de los parámetros de **Tfidf** y\n",
    "una elección con validación cruzada de los parámetros del clasificador acabarán elevando la precisión por encima del 90 por ciento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "Algoritmo popular no supervisado solo para texto **NLP** \n",
    "El objetivo del **LDA** es extraer conjuntos de palabras homogéneas, o temas,\n",
    "de una colección de documentos. Las matemáticas que hay detrás del\n",
    "algoritmo son muy avanzadas; aquí veremos sólo una noción práctica del\n",
    "mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document count: 1188\n",
      "\n",
      "I have a new doctor who gave me a prescription today for something called \n",
      "Septra DS.  He said it may cause GI problems and I have a sensitive stomach \n",
      "to begin with.  Anybody ever taken this antibiotic.  Any good?  Suggestions \n",
      "for avoiding an upset stomach?  Other tips?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['new', 'doctor', 'gave', 'prescription', 'today', 'called',\n",
       "       'septra', 'ds', 'said', 'cause', 'gi', 'problems', 'sensitive',\n",
       "       'stomach', 'begin', 'anybody', 'taken', 'antibiotic', 'good',\n",
       "       'suggestions', 'avoiding', 'upset', 'stomach', 'tips'],\n",
       "      dtype='<U12')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    tokenizar y crear una lista de todas las palabras incluidas en el conjunto de\n",
    "    datos. También elimina las palabras vacías y pone cada palabra en minúsculas\"\"\"\n",
    "\n",
    "    return [token.lower() for token in gensim.utils.simple_preprocess(text)\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS]\n",
    "\n",
    "\n",
    "text_dataset = fetch_20newsgroups(categories=['rec.autos', 'sci.med'],\n",
    "                                  random_state=101,\n",
    "                                  remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "\n",
    "documents = text_dataset.data\n",
    "print(\"Document count:\", len(documents))\n",
    "\n",
    "print(documents[0])\n",
    "np.array(tokenize(documents[0])).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 16161\n",
      "Dictionary<16161 unique tokens: ['antibiotic', 'anybody', 'avoiding', 'begin', 'called']...>\n"
     ]
    }
   ],
   "source": [
    "processed_docs = [tokenize(doc) for doc in documents]\n",
    "word_dic = gensim.corpora.Dictionary(processed_docs)\n",
    "print(\"Num tokens:\", len(word_dic))\n",
    "\n",
    "print(word_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el conjunto de datos hay algo más de 16.000 palabras distintas. Ahora\n",
    "toca filtrar las palabras demasiado comunes y las demasiado raras. En este\n",
    "paso, mantendremos las palabras que aparezcan al menos 10 veces y no\n",
    "más del 20% de los documentos. En este punto, tenemos la representación\n",
    "\"Bag Of Words\" (o BoW) de cada documento; es decir, cada documento se\n",
    "representa como un diccionario que contiene cuántas veces aparece cada\n",
    "palabra en el texto. La posición absoluta de cada palabra en el texto se\n",
    "pierde, exactamente igual que si metiéramos todas las palabras del\n",
    "documento en una bolsa. Como resultado, no toda la señal del texto se\n",
    "capta en las características basadas en este enfoque, pero la mayor parte\n",
    "de latiempo, basta con hacer un modelo eficaz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<1403 unique tokens: ['anybody', 'begin', 'called', 'cause', 'doctor']...>\n"
     ]
    }
   ],
   "source": [
    "#representación \"Bag Of Words\"  BoW) de cada documento\n",
    "word_dic.filter_extremes(no_below=10, no_above=0.2)\n",
    "print(word_dic)\n",
    "\n",
    "bow = [word_dic.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(\n",
    "    bow, num_topics=2, id2word=word_dic, passes=10, iterations=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"edu\" + 0.011*\"com\" + 0.009*\"people\" + 0.006*\"msg\" + 0.006*\"food\" + 0.006*\"patients\" + 0.006*\"time\" + 0.006*\"pain\" + 0.005*\"doctor\" + 0.005*\"disease\"'),\n",
       " (1,\n",
       "  '0.019*\"car\" + 0.008*\"new\" + 0.008*\"use\" + 0.007*\"cars\" + 0.007*\"good\" + 0.006*\"think\" + 0.006*\"time\" + 0.005*\"health\" + 0.005*\"engine\" + 0.005*\"year\"')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Con el modelo está entrenado, podemos ver la asociación entre palabras y temas\n",
    "lda_model.print_topics(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo no proporciona un nombre abreviado de los temas, sino su composición (los\n",
    "números son los pesos de cada palabra dentro de cada tema, ordenados\n",
    "de mayor a menor). Además, observe que algunas palabras aparecen en\n",
    "ambos temas; son palabras ambiguas que pueden utilizarse en ambos\n",
    "sentidos.\n",
    "\n",
    "Ahora veamos como se comporta el algoritmo con documentos no vistos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6015017032623291\t Topic: 0.019*\"car\" + 0.008*\"new\" + 0.008*\"use\" + 0.007*\"cars\" + 0.007*\"good\"\n",
      "Score: 0.3984982669353485\t Topic: 0.016*\"edu\" + 0.011*\"com\" + 0.009*\"people\" + 0.006*\"msg\" + 0.006*\"food\"\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"I've shown the doctor my new car. He loved its big wheels!\"\n",
    "\n",
    "bow_doc = word_dic.doc2bow(tokenize(new_doc))\n",
    "for index, score in sorted(lda_model[bow_doc], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las puntuaciones de ambos temas están en torno a 0,4 y 0,6, lo que\n",
    "significa que la frase contiene un buen equilibrio de los temas car y\n",
    "medicina. Lo que hemos mostrado aquí es sólo un ejemplo de dos temas;\n",
    "pero la misma implementación, gracias a la biblioteca de rendimiento\n",
    "**Gensim**, también puede asignar procesar toda la Wikipedia en inglés en\n",
    "cuestión de pocas horas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37964de45fa06bc99b754cfc54b483ffac9c133df6b16baa187ef33bd89a6318"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
