{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(boston.data,\n",
    "                                                    boston.target, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 3.8429092204444952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, Y_train)\n",
    "Y_pred = regr.predict(X_test)\n",
    "print(\"MAE\", mean_absolute_error(Y_test, Y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957 µs ± 154 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit regr.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir la variable objetivo en binaria(0 si el precio esta por debajo de la media)\n",
    "import numpy as np\n",
    "avg_price_house = np.average(boston.target)\n",
    "\n",
    "high_priced_idx = (Y_train >= avg_price_house)\n",
    "Y_train[high_priced_idx] = 1\n",
    "Y_train[np.logical_not(high_priced_idx)] = 0\n",
    "Y_train = Y_train.astype(np.int8)\n",
    "\n",
    "high_priced_idx = (Y_test >= avg_price_house)\n",
    "Y_test[high_priced_idx] = 1\n",
    "Y_test[np.logical_not(high_priced_idx)] = 0\n",
    "Y_test = Y_test.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85        61\n",
      "           1       0.82      0.68      0.75        41\n",
      "\n",
      "    accuracy                           0.81       102\n",
      "   macro avg       0.82      0.79      0.80       102\n",
      "weighted avg       0.81      0.81      0.81       102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manue!_PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data,\n",
    "                                                    iris.target, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay tres tipos de clasificadores Naive Bayes; cada uno de ellos tiene fuertes\n",
    "suposiciones (hipótesis) sobre las características. \n",
    "*   Si se trata de datos reales/continuos, el clasificador **gaussiano de Naive Bayes** asume que las\n",
    "características se generan a partir de un proceso gaussiano (es decir, que sedistribuyen normalmente).\n",
    "*   Si se trata de un modelo de eventos en el que éstos pueden modelarse con una distribución multinomial (en tal caso, las\n",
    "características son contadores o frecuencias), es necesario utilizar el clasificador **Naive Bayes multinomial**. \n",
    "*   si las características son independientes y booleanas, y es seguro asumir que son el resultado de un proceso\n",
    "Bernoulli, puede utilizar el clasificador **Bernoulli Naive Bayes**.\n",
    "\n",
    "\n",
    "Vamos a probar ahora un ejemplo de aplicación del clasificador gaussiano Naive\n",
    "Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      0.83      0.91         6\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.98      0.94      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print (classification_report(Y_test, Y_pred))\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbours\n",
    "Es uno de los métodos supervisados más sencillos, ya que la predicción se realiza con sólo mirar los K ejemplos más parecidos del conjunto de entrenamiento (en términos de distancia euclidiana o de algún otro tipo de distancia)\n",
    "\n",
    "Dosparámetros obligatorios para este algoritmo: la cardinalidad del\n",
    "vecindario (K), y la medida para evaluar la similitud (aunque la distancia\n",
    "euclidiana, o L2, es la más utilizada y es el parámetro por defecto para la\n",
    "mayoría de las implementaciones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos a utilizar un gran conjunto de datos, los dígitos manuscritos del MNIST. donde cada dijito esta compuesto por una matriz de 28 X 28 = 784 pixeles o caracteristicas\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# imports the mnist object previously pickled and dumped to disk\n",
    "import pickle\n",
    "mnist = pickle.load(open(\"resources\\mnist.pickle\", \"rb\"))\n",
    "\n",
    "# obtain 1000 random rows of data for reduce the dataset size, otherwise it'll take\n",
    "# too much time to run\n",
    "mnist.data, mnist.target = shuffle(mnist.data, mnist.target, random_state=101, n_samples=1000)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(mnist.data,\n",
    "                                                    mnist.target, test_size=0.4, random_state=0)\n",
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99        40\n",
      "         1.0       0.75      0.98      0.85        43\n",
      "         2.0       0.97      0.83      0.89        41\n",
      "         3.0       0.85      0.93      0.89        44\n",
      "         4.0       0.91      0.83      0.87        35\n",
      "         5.0       0.91      0.88      0.90        34\n",
      "         6.0       1.00      0.96      0.98        47\n",
      "         7.0       0.97      0.94      0.96        34\n",
      "         8.0       0.94      0.73      0.83        45\n",
      "         9.0       0.76      0.86      0.81        37\n",
      "\n",
      "    accuracy                           0.90       400\n",
      "   macro avg       0.90      0.89      0.90       400\n",
      "weighted avg       0.90      0.90      0.90       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# KNN: K=10, default measure of distance (euclidean)\n",
    "clf = KNeighborsClassifier(3)\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos uso de una busqueda aleatroria para buscar los mejores parametros de nuestro modelo K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manue!_PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.85666667 0.82166667 0.81833333 0.85666667 0.775      0.83666667\n",
      "        nan 0.675      0.80166667 0.82166667]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='ball_tree', n_neighbors=3, weights='distance')\n",
      "{'weights': 'distance', 'n_neighbors': 3, 'algorithm': 'ball_tree'}\n",
      "0.8566666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "h = KNeighborsClassifier()\n",
    "search_dict = {\n",
    "    'n_neighbors': [2, 3, 4, 5, 6, 10, 15, 20, 50, 100, 1000],\n",
    "    'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'weights': ['uniform', 'distance']}\n",
    "\n",
    "\n",
    "search_func = RandomizedSearchCV(estimator=h,\n",
    "                                 param_distributions=search_dict,\n",
    "                                 n_iter=10,\n",
    "                                 scoring='accuracy',\n",
    "                                 n_jobs=-1,\n",
    "                                 refit=True,\n",
    "                                 cv=10,\n",
    "                                 return_train_score=False)\n",
    "\n",
    "search_func.fit(X_train, Y_train)\n",
    "Y_pred = search_func.predict(X_test)\n",
    "# print(classification_report(Y_test, Y_pred))\n",
    "print(search_func.best_estimator_)\n",
    "print(search_func.best_params_)\n",
    "print(search_func.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37964de45fa06bc99b754cfc54b483ffac9c133df6b16baa187ef33bd89a6318"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
