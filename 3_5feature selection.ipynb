{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Selection**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection based on feature variance\n",
    "elimina todas las características que tienen una varianza pequeña; normalmente, menor que la establecida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.36033407 1.36888812 0.94894438 0.6457313  1.73496904]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# construir un conjunto artificial de datos\n",
    "X, y = make_classification(n_samples=10, n_features=5,\n",
    "                           n_informative=3, n_redundant=0, random_state=101)\n",
    "\n",
    "\n",
    "print(np.var(X, axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La menor varianza está asociada a la  característica 4; por lo tanto, si\n",
    "queremos seleccionar las cuatro mejores características, deberíamos fijar\n",
    "el umbral de varianza mínima en 0.9  Hagámoslo y veamos qué ocurre con\n",
    "la primera observación del conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: [-2.02411927 -1.66432922 -0.82764946  0.05353877  0.34858907]\n",
      "After:  [-2.02411927 -1.66432922 -0.82764946  0.34858907]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X_selected = VarianceThreshold(threshold=0.9).fit_transform(X)\n",
    "print (\"Before:\", X[0, :])\n",
    "print (\"After: \", X_selected[0, :]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección univariante\n",
    "se pretende seleccionar lasvariables individuales que más se asocian con su variable objetivo según\n",
    "una prueba estadística.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.84933815,  0.48925586,  0.72980944],\n",
       "       [ 1.36658414,  2.04664397,  0.17731243],\n",
       "       [ 1.64266792, -0.43187942, -0.90353033],\n",
       "       [ 1.23495751, -2.76593955,  0.06069002]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=800, n_features=100, n_informative=25,\n",
    "                           n_redundant=0, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  6,  7, 12, 30, 34, 35, 38, 45, 54, 56, 59, 61, 66, 72, 76, 80,\n",
       "        81, 84, 89, 99], dtype=int64),)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "from sklearn.preprocessing import Binarizer, scale\n",
    "import numpy as np\n",
    "\n",
    "# En chi-cuadrado la entrada X debe ser no negativa(debe contener booleanos ofrecuencias), de\n",
    "#  ahí la elección de binarizar luego de normalizar si la variable está por encima de la media.\n",
    "Xbin = Binarizer().fit_transform(scale(X))\n",
    "\n",
    "Selector_chi2 = SelectPercentile(chi2, percentile=25).fit(Xbin, y)\n",
    "Selector_f_classif = SelectPercentile(f_classif, percentile=25).fit(X, y)\n",
    "\n",
    "chi_scores = Selector_chi2.get_support()\n",
    "f_classif_scores = Selector_f_classif.get_support()\n",
    "\n",
    "# seleccionar las caracteristicas que estan en los dos test\n",
    "selected = chi_scores & f_classif_scores  # use the bitwise and operator\n",
    "\n",
    "selected.nonzero()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection based on recursive elimination\n",
    "En cada iteracion va podando variables que mantengan estable o aumenten la puntuacion que se este utilizando.cuando se observa una gran discrepancia entre los resultados de entrenamiento (basados en la validación cruzada, no en la puntuación\n",
    "dentro de la muestra) y los resultados fuera de la muestra, **la selección recursiva** puede ayudar a conseguir un mejor rendimiento de los algoritmos de aprendizaje, señalando algunas de las variables más importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, n_features=100,\n",
    "                           n_informative=5, n_redundant=2, random_state=101)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample accuracy: 1.000\n",
      "Out-of-sample accuracy: 0.767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=101)\n",
    "classifier.fit(X_train, y_train)\n",
    "print ('In-sample accuracy: %0.3f' % classifier.score(X_train, y_train))\n",
    "print ('Out-of-sample accuracy: %0.3f' % classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features : 33\n",
      "Out-of-sample accuracy: 0.700\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "classifier = LogisticRegression(random_state=101)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "selector = RFECV(estimator=classifier, step=1, cv=10, scoring='accuracy')\n",
    "selector.fit(X_train, y_train)\n",
    "print('Optimal number of features : %d' % selector.n_features_)\n",
    "\n",
    "# trasformar los datos ya con las cararacteristica sseleccionadas\n",
    "X_train_s = selector.transform(X_train)\n",
    "X_test_s = selector.transform(X_test)\n",
    "classifier.fit(X_train_s, y_train)\n",
    "print('Out-of-sample accuracy: %0.3f' %\n",
    "      classifier.score(X_test_s, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37964de45fa06bc99b754cfc54b483ffac9c133df6b16baa187ef33bd89a6318"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
