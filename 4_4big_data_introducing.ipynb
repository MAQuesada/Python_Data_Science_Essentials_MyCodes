{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un ejemplo genérico de clasificación,\n",
    "crearemos un conjunto de datos sintéticos que contiene 10 millones de casos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import  numpy as  np\n",
    "\n",
    "X,y = make_classification(n_samples=10**7, n_features=5, n_informative=3, random_state=101)\n",
    "D = np.c_[y,X]\n",
    "#the saved file should be around 1,46 GB\n",
    "np.savetxt('resources/large_dataset_10__7.csv', D, delimiter=\",\") \n",
    "\n",
    "# liberar la memoria \n",
    "del(D, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El truco para gestionar grandes volúmenes de datos sin cargar demasiados\n",
    "megabytes (o gigabytes) de datos en la memoria es actualizar de forma\n",
    "incremental los parámetros del algoritmo utilizando sólo una parte de los\n",
    "ejemplos cada vez, esto es posible en **Scikit-learn** gracias al método *.partial_fit()*, que se ha\n",
    "puesto a disposición de un cierto número de algoritmos supervisados y no\n",
    "supervisados. Este método se deonoma **incremental learning** \n",
    "  \n",
    "* Los trozos de datos que se introducen de forma incremental en el algoritmo de aprendizaje se\n",
    "denominan lotes (batches). El tamaño de los lotes depende generalmente de la memoria disponible(cuanto mayores sean los trozos de datos, mejor, ya que la muestra de datos obtendrá más representantes de las distribuciones de\n",
    "datos).\n",
    " \n",
    "* Los algoritmos de aprendizaje incremental funcionan bien con datos en el intervalo de [-1,+1] o [0,+1] (por ejemplo,\n",
    "Multinomial Bayes no acepta valores negativos). Sin embargo, para escalar a un rango tan preciso, es necesario conocer de antemano el rango de cada variable. Como alternativa, tiene que hacer una de estas cosas: pasar todos los\n",
    "datos de una vez, registrar los valores mínimo y máximo, o derivarlos del primer lote, recortando las observaciones siguientes que superen los valores máximo y mínimo iniciales.\n",
    "\n",
    "* La validación es un flujo de lotes, que puede conseguirse de dos maneras: -Validar de forma progresiva; es decir, probar primero cómo predice el modelo los trozos de datos recién llegados antes de pasarlos a\n",
    "entrenamiento. -Reserve algunas observaciones de cada trozo. Esta\n",
    "última es también la mejor forma de reservar una muestra para la\n",
    "*GridSearchCV()* o para alguna otra optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro ejemplo, confiamos al **SGDClassifier** una pérdida logarítmica(análoga a una regresión logística) para que aprenda a predecir un resultado binario dado 10**7 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressive validation mean accuracy 0.708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# read the big data in chunk with 10000 of length\n",
    "streaming = pd.read_csv('resources/large_dataset_10__7.csv', header=None, chunksize=10000)\n",
    "\n",
    "learner = SGDClassifier(loss='log_loss', max_iter=100)\n",
    "\n",
    "# Utilizamos el MinMaxScaler para registrar el rango de cada variable en el primer lote.\n",
    "minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "cumulative_accuracy = list()\n",
    "\n",
    "for n, chunk in enumerate(streaming):\n",
    "    if n == 0:\n",
    "        minmax_scaler.fit(chunk.iloc[:, 1:].values)\n",
    "    X = minmax_scaler.transform(chunk.iloc[:, 1:].values)\n",
    "    # utilizaremos la regla de que si supera uno de los límites[0,+1], se recortan al más cercano.\n",
    "    X[X > 1] = 1\n",
    "    X[X < 0] = 0\n",
    "\n",
    "    y = chunk.iloc[:, 0]\n",
    "\n",
    "   # a partir del décimo lote, registraremos la precisión en cada lote\n",
    "    if n > 8:\n",
    "        cumulative_accuracy.append(learner.score(X, y))\n",
    "    learner.partial_fit(X, y, classes=np.unique(y))\n",
    "\n",
    "print('Progressive validation mean accuracy %0.3f' % np.mean(cumulative_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varios algoritmos funcionan utilizando el aprendizaje incremental. Para la\n",
    "clasificación:\n",
    "* **sklearn.naive_bayes.BernoulliNB**\n",
    "* **sklearn.naive_bayes.MultinomialNB**\n",
    "* **sklearn.linear_model.Perceptron**\n",
    "* **sklearn.linear_model.SGDClassifier**\n",
    "* **sklearn.linear_model.PassiveAggressiveClassifier**\n",
    " \n",
    "Para la regresión:\n",
    "* **sklearn.linear_model.SGDRegressor**\n",
    "* **sklearn.linear_model.PassiveAggressiveRegressor**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37964de45fa06bc99b754cfc54b483ffac9c133df6b16baa187ef33bd89a6318"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
