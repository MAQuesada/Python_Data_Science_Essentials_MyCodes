{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dealing with big datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el conjunto de datos a cargar es demasiado grade para tenerlo en memoria podemos cargarlo por lotes o por trozos. Con **pandas** tenemos dos formas de hacerlo: \n",
    "* cargando el conjunto de datos en trozos del mismo tamaño\n",
    "* pedir un iterador al conjunto de datos que permite pedir dinamicamanmete la longitud de cada trozo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10, 5)  --  Shape: (10, 5)  --  Shape: (10, 5)  --  Shape: (10, 5)  --  Shape: (10, 5)  --  Shape: (10, 5)  --  Shape: (10, 5)  --  Shape: (10, 5)  --  Shape: (10, 5)  --  Shape: (10, 5)  --  Shape: (10, 5)  --  Shape: (10, 5)  --  Shape: (10, 5)  --  Shape: (10, 5)  --  Shape: (10, 5)  --  "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "iris_chunks = pd.read_csv('resources\\datasets-uci-iris.csv', header=None,\n",
    "                          names=['C1', 'C2', 'C3', 'C4', 'C5'],\n",
    "                          chunksize=10)\n",
    "for chunk in iris_chunks:\n",
    "    print('Shape:', chunk.shape, end='  --  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n",
      "(20, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.8, 3.1, 1.6, 0.2, 'Iris-setosa'],\n",
       "       [5.4, 3.4, 1.5, 0.4, 'Iris-setosa']], dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_iterator = pd.read_csv('resources\\datasets-uci-iris.csv', header=None,\n",
    "                            names=['C1', 'C2', 'C3', 'C4', 'C5'],\n",
    "                            iterator=True)\n",
    "print(iris_iterator.get_chunk(10).shape)\n",
    "print(iris_iterator.get_chunk(20).shape)\n",
    "iris_iterator.get_chunk(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la lectura de los datos desde CSV consume mucho tiempo y requiere de gran esfuerzo cada vez(establecer los tipos y los nombres de las variables correctas) podemos acelerar ese guardado y cargado usando la estructura de datos **HDF5** que es es un conjunto de tecnología único que hace posible la gestión de colecciones de datos extremadamente grandes y complejas. Se organiza como almacenamiento gerarquicos de datos que permite guardar matrices multidimensionales de un tipo o grupo homogéneo que son contenedores de matrices y otros grupos y se ajustan perfectamente a los DataFrame mediante la compresión automatica de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciar el archivo\n",
    "storage = pd.HDFStore('resources\\example.h5')\n",
    "\n",
    "# guardar en él como si fuera un diccionario\n",
    "iris = pd.read_csv(\n",
    "    'resources\\datasets-uci-iris.csv',\n",
    "    names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'target'])\n",
    "storage['iris'] = iris\n",
    "storage.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/iris']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para recuperar los archivos\n",
    "storage = pd.HDFStore('resources\\example.h5')\n",
    "\n",
    "# verificar las llaves que hay\n",
    "print(storage.keys())\n",
    "\n",
    "fast_iris_upload = storage['iris']\n",
    "type(fast_iris_upload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Use SQLite to munging data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# crear las consultas para eliminar y crear una tabla\n",
    "drop_query = \"DROP TABLE IF EXISTS temp_data;\"\n",
    "create_query = \"CREATE TABLE temp_data \\\n",
    "(date INTEGER, city VARCHAR(80), \\\n",
    "temperature REAL, destination INTEGER);\"\n",
    "\n",
    "# conectar con DB o crearla si no existe\n",
    "connection = sqlite3.connect(\"resources\\example.db\")\n",
    "connection.execute(drop_query)\n",
    "connection.execute(create_query)\n",
    "connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insertar los datos en la DB\n",
    "data = [(20140910, \"Rome\", 80.0, 0),\n",
    "        (20140910, \"Berlin\", 50.0, 0),\n",
    "        (20140910, \"Wien\", 32.0, 1),\n",
    "        (20140911, \"Paris\", 65.0, 0)]\n",
    "insert_query = \"INSERT INTO temp_data VALUES(?, ?, ?, ?)\"\n",
    "connection.executemany(insert_query, data)\n",
    "connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city</th>\n",
       "      <th>temperature</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20140910</td>\n",
       "      <td>Rome</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20140910</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20140910</td>\n",
       "      <td>Wien</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date    city  temperature  destination\n",
       "0  20140910    Rome         80.0            0\n",
       "1  20140910  Berlin         50.0            0\n",
       "2  20140910    Wien         32.0            1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection_query = \"SELECT date, city, temperature, destination FROM temp_data WHERE Date=20140910\"\n",
    "retrieved = pd.read_sql_query(selection_query, connection)\n",
    "connection.close()\n",
    "retrieved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **create a mask and apply for many functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">sepal_length</th>\n",
       "      <th colspan=\"2\" halign=\"left\">sepal_width</th>\n",
       "      <th colspan=\"2\" halign=\"left\">petal_length</th>\n",
       "      <th colspan=\"2\" halign=\"left\">petal_width</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Iris-setosa</th>\n",
       "      <td>5.006</td>\n",
       "      <td>0.352490</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.464</td>\n",
       "      <td>0.173511</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <td>5.936</td>\n",
       "      <td>0.516171</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.260</td>\n",
       "      <td>0.469911</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New label</th>\n",
       "      <td>6.588</td>\n",
       "      <td>0.635880</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>5.552</td>\n",
       "      <td>0.551895</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sepal_length           sepal_width      petal_length  \\\n",
       "                        mean       std         max  min         mean   \n",
       "target                                                                 \n",
       "Iris-setosa            5.006  0.352490         4.4  2.3        1.464   \n",
       "Iris-versicolor        5.936  0.516171         3.4  2.0        4.260   \n",
       "New label              6.588  0.635880         3.8  2.2        5.552   \n",
       "\n",
       "                          petal_width       \n",
       "                      std         max  min  \n",
       "target                                      \n",
       "Iris-setosa      0.173511         0.6  0.1  \n",
       "Iris-versicolor  0.469911         1.8  1.0  \n",
       "New label        0.551895         2.5  1.4  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask example\n",
    "from doctest import Example\n",
    "\n",
    "\n",
    "mask_target = iris['target'] == 'Iris-virginica'\n",
    "iris.loc[mask_target, 'target'] = 'New label'\n",
    "\n",
    "# Example of the Aplicate many statistics\n",
    "funcs = {'sepal_length': ['mean', 'std'],\n",
    "         'sepal_width': ['max', 'min'],\n",
    "         'petal_length': ['mean', 'std'],\n",
    "         'petal_width': ['max', 'min']}\n",
    "iris.groupby(['target']).agg(funcs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Aplicar  funciones e los datos en pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  target\n",
       "0             3            3             3            3      11\n",
       "1             3            3             3            3      11\n",
       "2             3            3             3            3      11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''operar elemento por elemnto con applymap()'''\n",
    "# obtenr eltamaño de cadena de cada columna\n",
    "iris.applymap(lambda x: len(str(x))).head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.01</td>\n",
       "      <td>12.25</td>\n",
       "      <td>1.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.01</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.09</td>\n",
       "      <td>10.24</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length\n",
       "0         26.01        12.25          1.96\n",
       "1         24.01         9.00          1.96\n",
       "2         22.09        10.24          1.69"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "culumns_to_modif = ['sepal_length', 'sepal_width', 'petal_length']\n",
    "iris[culumns_to_modif].apply(lambda x: x**2).head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Uno delos putos débiles es que estas transformaciones pueden demorar mucho tiempo pq la biblioteca no aprovecha las apacidades de multiprocesamiento de las CPU modernas. Por eso para grandes volumenes de datos debemos hacerlo nosotros mismo como se muestra en el script:  ``` **2__multiprocessing.py** ya que el portátil Jupyter no puede ejecutar multiprocesamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Processing Text. Usage Bag(Bolsa) of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: flb@flb.optiplan.fi (\"F.Baube[tm]\")\n",
      "Subject: Vandalizing the sky\n",
      "X-Added: Forwarded by Space Digest\n",
      "Organization: [via International Space University]\n",
      "Original-Sender: isu@VACATION.VENARI.CS.CMU.EDU\n",
      "Distribution: sci\n",
      "Lines: 12\n",
      "\n",
      "From: \"Phil G. Fraering\" <pgf@srl03.cacs.usl.edu>\n",
      "> \n",
      "> Finally: this isn't the Bronze Age, [..]\n",
      "> please try to remember that there are more human activities than\n",
      "> those practiced by the Warrior Caste, the Farming Caste, and the\n",
      "> Priesthood.\n",
      "\n",
      "Right, the Profiting Caste is blessed by God, and may \n",
      " freely blare its presence in the evening twilight ..\n",
      "\n",
      "-- \n",
      "* Fred Baube (tm)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# colección de aproximadamente 20 000 documentos de grupos de noticias, divididos (casi)\n",
    "# uniformemente en 20 grupos de noticias diferentes\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['sci.med', 'sci.space']\n",
    "twenty_sci_news = fetch_20newsgroups(categories=categories)\n",
    "print(twenty_sci_news.data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix is sparse de dimensiones :  (1187, 25638)\n",
      "Word \"from\" appears 2 times         Word \"flb\" appears 2 times         Word \"optiplan\" appears 1 times         Word \"fi\" appears 1 times         Word \"baube\" appears 2 times         Word \"tm\" appears 2 times         Word \"subject\" appears 1 times         Word \"vandalizing\" appears 1 times         Word \"the\" appears 7 times         Word \"sky\" appears 1 times         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manue!_PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "'''transformar cada documento en un conjunto de palabras(caracteristicas)'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "word_count = count_vect.fit_transform(twenty_sci_news.data)\n",
    "print('matrix is sparse de dimensiones : ', word_count.shape)\n",
    "\n",
    "# ver el primer documento ( solo las 10 primeras palabras)\n",
    "word_list = count_vect.get_feature_names()\n",
    "for n in word_count[0].indices[:10]:\n",
    "    print('Word \"%s\" appears %i times' % (word_list[n], word_count[0, n]), end='         ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \"from\" has frequency 0.022         Word \"flb\" has frequency 0.022         Word \"optiplan\" has frequency 0.011         Word \"fi\" has frequency 0.011         Word \"baube\" has frequency 0.022         Word \"tm\" has frequency 0.022         Word \"subject\" has frequency 0.011         Word \"vandalizing\" has frequency 0.011         Word \"the\" has frequency 0.077         Word \"sky\" has frequency 0.011         "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999999999999987"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''calcular las frecuenciuas de cada palabra'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tf_vect = TfidfVectorizer(use_idf=False, norm='l1')\n",
    "word_freq = tf_vect.fit_transform(twenty_sci_news.data)\n",
    "word_list = tf_vect.get_feature_names()\n",
    "for n in word_freq[0].indices[:10]:\n",
    "    print('Word \"%s\" has frequency %0.3f' % (word_list[n], word_freq[0, n]), end='         ')\n",
    "\n",
    "word_freq[0].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conmo vemos si la norma es 'L1' la suma de las frecuencias va a ser igual a uno. si queremos aumentar la diferencia entre palabras raras y las comunes podemos usar 'l2' entonces la suma de los cuadrados de las frecuencias obtenidas va a ser igual a uno. lo podemos comprobar  con \n",
    "```python\n",
    "np.power(word_freq[0].todense(),2).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \"fred\" has tf-idf 0.089         Word \"twilight\" has tf-idf 0.139         Word \"evening\" has tf-idf 0.113         Word \"in\" has tf-idf 0.024         Word \"presence\" has tf-idf 0.119         Word \"its\" has tf-idf 0.061         Word \"blare\" has tf-idf 0.150         Word \"freely\" has tf-idf 0.119         Word \"may\" has tf-idf 0.054         Word \"god\" has tf-idf 0.119         "
     ]
    }
   ],
   "source": [
    "'''uso del parametro 'use_idf'  (significa término-frecuencia multiplicado por la inversa de la frecuencia del documento)  que permite destacar palabras q describen eficazmente cada documento'''\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()  # Default: use_idf=True\n",
    "\n",
    "word_tfidf = tfidf_vect.fit_transform(twenty_sci_news.data)\n",
    "word_list = tfidf_vect.get_feature_names()\n",
    "for n in word_tfidf[0].indices[:10]:\n",
    "    print('Word \"%s\" has tf-idf %0.3f' % (word_list[n], word_tfidf[0, n]), end='         ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asi d esta manera las palabras con frecuencias altas en el documento son raras en el resto de los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word list =  ['data', 'data science', 'hard', 'is', 'is hard', 'love', 'love data', 'science', 'science is', 'we', 'we love']\n",
      "text_1 is described with ['we(1)', 'love(1)', 'data(1)', 'science(1)', 'we love(1)', 'love data(1)', 'data science(1)']\n"
     ]
    }
   ],
   "source": [
    "'''usando n-grams donde la presencia o ausencia de una palabra asi como de sus vecinas importa'''\n",
    "\n",
    "documents = np.array(['we love data science', 'data science is hard'])\n",
    "\n",
    "# uni- and bi-gram count vectorizer for only use bi-gram with (2,2)\n",
    "count_vect_1_grams = CountVectorizer(ngram_range=(1, 2))\n",
    "word_count = count_vect_1_grams.fit_transform(documents)\n",
    "word_list = count_vect_1_grams.get_feature_names()\n",
    "print(\"Word list = \", word_list)\n",
    "print(\n",
    "    \"text_1 is described with\",\n",
    "    [word_list[n] + \"(\" + str(word_count[0, n]) + \")\" for n in word_count[0].indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El número de características se dispara exponencialmente cuando usamos **n-grams**. si se tienen demasiadas caracteristicas es habitual usar el truco del Hashing en el que se hace un **bucket of words** y sus hash colisionan. Los **bucket** son conjuntos de palabras no relacionadas semanticamente pero con hashes que colisionan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.  , -0.5 ,  0.  ,  0.25,  0.  ,  0.  ,  0.  ,  0.25],\n",
       "        [ 0.  , -0.5 ,  0.  ,  0.5 ,  0.  ,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hash_vect = HashingVectorizer(n_features=8, norm='l1')\n",
    "word_hashed = hash_vect.fit_transform(documents)\n",
    "word_hashed.todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta que no puedes invertir el proceso de hashing (ya que es una operación de digestión).Por lo tanto, después de esta transformación, usted tendrá que trabajar en el hashed tal y como están. \n",
    "\n",
    "El hashing presenta bastantes ventajas: permite transformar rápidamente una bolsa de palabras en vectores de características (los bucket de hash son nuestras características, en este caso) y evitar el sobreajuste por de palabras no relacionadas entre sí en la misma característica. Ademas es de memoria muy baja escalable a grandes conjuntos de datos ya que no hay necesidad de almacenar un diccionario de vocabulario en la memoria.\n",
    "\n",
    "También hay un par de contras (en comparación con el uso de un CountVectorizer con un vocabulario en memoria): no hay forma de calcular la transformada inversa (desde índices de características hasta nombres de características de cadena), lo que puede ser un problema cuando se intenta introspeccionar qué características son más importantes para un modelo. Puede haber colisiones: distintos tokens se pueden asignar al mismo índice de características. Sin embargo, en la práctica, esto rara vez es un problema si n_features es lo suficientemente grande (por ejemplo, 2 ** 18 para problemas de clasificación de texto) y ademas no tiene ponderación IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scraping(raspado) the web with BeautifulSoup**\n",
    "Es un tema muy extenso al que se le pudiera dedicar practicamente un libro, por eso para aplicarlo hay q buscar mas informacion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# 1. Let's download the code behind the William Shakespeare page on Wikipedia:\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/William_Shakespeare'\n",
    "request = urllib.request.Request(url)\n",
    "response = urllib.request.urlopen(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>William Shakespeare - Wikipedia</title>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 2. leer el y parsear el URL con analizaforHTML\n",
    "\n",
    "soup = BeautifulSoup(response, 'html.parser')\n",
    "soup.title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero debemos analizar manualmente la propia página HTML para averiguar cuál es la mejor etiqueta HTML que contiene la\n",
    "información que buscamos. Tras un **análisis manual**, descubrimos que las categorías están dentro de un div llamado\n",
    "mw-normal-catlinks'; excluyendo el primer enlace, todos los demás están bien. Ahora, es\n",
    "es hora de programar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:William Shakespeare -> /wiki/Category:William_Shakespeare\n",
      "Category:1564 births -> /wiki/Category:1564_births\n",
      "Category:1616 deaths -> /wiki/Category:1616_deaths\n",
      "Category:16th-century English male actors -> /wiki/Category:16th-century_English_male_actors\n",
      "Category:English male stage actors -> /wiki/Category:English_male_stage_actors\n",
      "Category:16th-century English dramatists and playwrights -> /wiki/Category:16th-century_English_dramatists_and_playwrights\n",
      "Category:17th-century English dramatists and playwrights -> /wiki/Category:17th-century_English_dramatists_and_playwrights\n",
      "Category:16th-century English poets -> /wiki/Category:16th-century_English_poets\n",
      "Category:Burials in Warwickshire -> /wiki/Category:Burials_in_Warwickshire\n",
      "Category:17th-century English poets -> /wiki/Category:17th-century_English_poets\n",
      "Category:17th-century English male writers -> /wiki/Category:17th-century_English_male_writers\n",
      "Category:English Renaissance dramatists -> /wiki/Category:English_Renaissance_dramatists\n",
      "Category:People educated at King Edward VI School, Stratford-upon-Avon -> /wiki/Category:People_educated_at_King_Edward_VI_School,_Stratford-upon-Avon\n",
      "Category:People from Stratford-upon-Avon -> /wiki/Category:People_from_Stratford-upon-Avon\n",
      "Category:People of the Elizabethan era -> /wiki/Category:People_of_the_Elizabethan_era\n",
      "Category:Shakespeare family -> /wiki/Category:Shakespeare_family\n",
      "Category:Sonneteers -> /wiki/Category:Sonneteers\n",
      "Category:King's Men (playing company) -> /wiki/Category:King%27s_Men_(playing_company)\n",
      "Category:17th-century English male actors -> /wiki/Category:17th-century_English_male_actors\n",
      "Category:English male dramatists and playwrights -> /wiki/Category:English_male_dramatists_and_playwrights\n",
      "Category:English male poets -> /wiki/Category:English_male_poets\n"
     ]
    }
   ],
   "source": [
    "section = soup.find_all(id='mw-normal-catlinks')[0]\n",
    "for catlink in section.find_all(\"a\")[1:]:\n",
    "    print(catlink.get(\"title\"), \"->\", catlink.get(\"href\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data processing with NumPy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Create ndArray**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "list_of_ints = [1, 2, 3]\n",
    "Array_1 = np.array(list_of_ints, dtype=np.int8)\n",
    "\n",
    "print(type(Array_1))\n",
    "Array_1.nbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como los ndArray tienen tamaño fijo es muy importante que el tipo de datos sea el adecuado para nuestros datos ya que nos ayudará a ahorrar memoria por ejemplo : \n",
    "* int 4B Default integer type (normally int32or int64)\n",
    "* int8          ----------->1Bytes------------   Byte (-128 to 127)\n",
    "* int16         ----------->2Bytes------------   Integer (-32768 to 32767)\n",
    "* int32         ----------->4Bytes------------   Integer (-2** 31 to 2** 31-1)\n",
    "* int64         ----------->8Bytes------------   Integer (-2** 63 to 2** 63-1)\n",
    "* uint8         ----------->1Bytes------------   Unsigned integer (0 to 255)\n",
    "* uint16        ----------->2Bytes------------   Unsigned integer (0 to 65535)\n",
    "* uint32        ----------->4Bytes------------   Unsigned integer (0 to 2**32-1)\n",
    "* uint64        ----------->8Bytes------------   Unsigned integer (0 to 2**64-1)\n",
    "* float16       ----------->2Bytes------------   Half-precision float (exponent 5 bits, mantissa 10 bits)\n",
    "* float_        ----------->8Bytes------------   Shorthand for float64\n",
    "* float32       ----------->4Bytes------------   Single-precision float (exponent 8 bits, mantissa 23 bits)\n",
    "* float64       ----------->8Bytes------------   Double-precision float (exponent 11 bits,mantissa 52 bits)\n",
    "\n",
    "\n",
    "Para cambiar el tipo de de un ndArray se utiliza    \n",
    "```python\n",
    "Array_1.astype('float32')\n",
    "float types pevalece sobre tipos int,y strings (<U32means a Unicode string of size 32 or less)predominan a todos los demas tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(dict_items([(1, 2), (3, 4), (5, 6)]), dtype=object)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if a NumPy array is of the desired numeric type\n",
    "print(isinstance(Array_1[0], np.number))\n",
    "\n",
    "# crear un ndArray a partir de un diccionario\n",
    "np.array({1: 2, 3: 4, 5: 6}.items())\n",
    "\n",
    "# Para crearlo mediante un DataFrame se utiliza el atributo df.values \n",
    "# Se recomienda antes de hacer esto verificar los tipos en cada columna y si no\n",
    "# son homogeneos realizar alguna trasnformacion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Resizing arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array  modificado pq es una vista del original\n",
      " [[-1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]]\n",
      "array sin modificar pq es una copia\n",
      " [[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Restructuring a NumPy array shape\n",
    "original_array = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "Array_a = original_array.reshape(4, 2)\n",
    "Array_b = original_array.reshape(4, 2).copy()\n",
    "Array_c = original_array.reshape(2, 2, 2)\n",
    "\n",
    "# Attention because reshape creates just views, not copies\n",
    "original_array[0] = -1\n",
    "\n",
    "print('array  modificado pq es una vista del original\\n', Array_a)\n",
    "print('array sin modificar pq es una copia\\n', Array_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  2],\n",
       "       [ 3,  4],\n",
       "       [ 5,  6],\n",
       "       [ 7,  8]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para cambiar las diemnsiones del array original se utiliza\n",
    "original_array.resize(4, 2)\n",
    "original_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  2,  3,  4,  5,  6,  7,  8]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resize es equivalente a\n",
    "original_array.shape = (1, 8)\n",
    "original_array.shape = (1, -1)  # ya que -1 re refiere al ultimo elemento\n",
    "\n",
    "original_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Arrays derived from NumPy functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2812499 , 0.05111399, 0.97705041, 0.27080453],\n",
       "       [0.09449049, 0.50801466, 0.29855802, 0.11840719]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uso de arange e invirtiendo el arreglo con el operador de slice\n",
    "np.arange(9)[::-1]\n",
    "\n",
    "# Enteros aleatrorios y posiblemente repetidos\n",
    "np.random.randint(low=1, high=10, size=(2, 4))\n",
    "\n",
    "np.zeros((3, 3))     # matriz nula\n",
    "\n",
    "np.ones((3, 3))      # matriz identidad\n",
    "\n",
    "np.eye(3)           # matriz diagonal\n",
    "\n",
    "# matriz fraccionaria del tamaño especificado y en el intervalo cerrado (0,2)\n",
    "fraction = np.linspace(start=0, stop=2, num=5)\n",
    "\n",
    "# matriz fraccionaria del tamaño  especificado y en el intervalo cerrado (base**start, base**stop)\n",
    "np.logspace(start=0, stop=2, num=5, base=10.0)\n",
    "\n",
    "# matriz de valores aleatorios que siguen la distribucion normal (mean=0, std=1)\n",
    "gaussian = np.random.normal(loc=0, scale=1, size=(2, 4))\n",
    "\n",
    "# matriz de valores aleatorios que siguen la distribucion uniforme (low=0.0, high=1.0)\n",
    "np.random.uniform(low=0.0, high=1.0, size=(2, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Getting an array directly from a file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ejecutaramos este comando:\n",
    "```python \n",
    "housing = np.loadtxt(\"resources/datasets-uci-iris.csv\", delimiter=',', dtype=float)\n",
    "``` \n",
    "nos daria una error similar a este :*ValueError: could not convert string to float: Iris-setosa*. una solucion seria si sabemos en que columna esta el problema entonces aplicarle una conversion mediante una funcion como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2, 0. ],\n",
       "       [4.9, 3. , 1.4, 0.2, 0. ],\n",
       "       [4.7, 3.2, 1.3, 0.2, 0. ],\n",
       "       [4.6, 3.1, 1.5, 0.2, 0. ],\n",
       "       [5. , 3.6, 1.4, 0.2, 0. ]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def from_txt_to_iris_class(x):\n",
    "    if x == b'Iris-setosa':\n",
    "        return 0\n",
    "    elif x == b'Iris-versicolor':\n",
    "        return 1\n",
    "    elif x == b'Iris-virginica':\n",
    "        return 2\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "np.loadtxt(\"resources/datasets-uci-iris.csv\", delimiter=',',\n",
    "           converters={4: from_txt_to_iris_class})[:5,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Aplicar una funcion a aun ndArray**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 1.        , 2.82842712, 5.19615242, 8.        ])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cube_power_square_root(x):\n",
    "    return np.sqrt(np.power(x, 3))\n",
    "    \n",
    "np.apply_along_axis(cube_power_square_root, \n",
    "axis=0, arr=np.arange(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Matrix operations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 3.5])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from re import S\n",
    "from wsgiref import simple_server\n",
    "\n",
    "\n",
    "a = np.array((1, 2, 3))\n",
    "b = np.array((2, 3, 4))\n",
    "\n",
    "# formar una matriz concatenado cada arreglo como si fueran una columna\n",
    "np.column_stack((a, b))\n",
    "\n",
    "# matriz bidimensional de 0 a 3\n",
    "M = np.arange(2*2, dtype=float).reshape(2, 2)\n",
    "\n",
    "coefs = np.array([1., 0.5])\n",
    "\n",
    "# producto de dos matrices\n",
    "np.dot(M, coefs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.  9.]\n",
      " [13. 14.]\n",
      " [18. 19.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([15., 16., 17., 18., 19.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crear indices booleanos para saber que filas y columnas queremos \n",
    "M = np.arange(5*5, dtype=float).reshape(5, 5)\n",
    "\n",
    "row_index = (M[:,0]>=5) & (M[:,0]<=15)  # solo las filas 2,3,4\n",
    "col_index = M[0,:]>=3                   # solo las dos ultimas columnas \n",
    "\n",
    "# notar como no podemos aplicar los indicies boolesnos dentro del mismo [], por eso se \n",
    "# filtra primero las filas y luego las columnas con 2 corchetes\n",
    "print(M[row_index,:][:,col_index] )\n",
    "\n",
    "mask = (M>=15) & (M<=20) & ((M / 10.) % 1 >= 0.5)\n",
    "M[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fancy indexing(indexacion de lujo)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  7., 14.])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Secuancia de indices que pueden estar desordenados e incluso repetidos para obtener\n",
    "# los elementos  ( 1,0),(1,2),(2,4)\n",
    "row_index = [1,1,2]\n",
    "col_index = [0,2,4]\n",
    "M[row_index,col_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  7.,  9.],\n",
       "       [ 5.,  7.,  9.],\n",
       "       [10., 12., 14.]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tambien sirve para seleccioanr filas ycolumnas que deseemos aplicamos primero la indexacion\n",
    "# por fila y luego por columna\n",
    "M[row_index, :][:, col_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recuerde que la segmentación y la indexación son solo vistas de los datos y cualquier cambio en el **ndArray original** se mostrará en ellos. Sino desea esto tiene que hacer una copia con el metodo **copy***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stacking NumPy arrays(Añadir filas y columnas)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4],\n",
       "       [ 5,  6,  7,  8,  9],\n",
       "       [10, 11, 12, 13, 14],\n",
       "       [15, 16, 17, 18, 19],\n",
       "       [20, 21, 22, 23, 24],\n",
       "       [ 0,  1,  2,  3,  4],\n",
       "       [ 0,  1,  2,  3,  4]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nuestro conjunto de datos\n",
    "dataset = np.arange(5*5).reshape(5, 5)\n",
    "\n",
    "single_line = np.arange(1*5).reshape(1, 5)\n",
    "a_few_lines = np.arange(3*5).reshape(3, 5)\n",
    "\n",
    "# añadir una sola fila\n",
    "np.vstack((dataset, single_line))\n",
    "\n",
    "# añadir varias filas\n",
    "np.vstack((dataset,a_few_lines))\n",
    "\n",
    "#añadiendo todas las filas que querramos al parametro que es una tupla donde el \n",
    "# primer elemento es el arreglo al que se le van a añadir las filas\n",
    "np.vstack((dataset,single_line,single_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.,  4.,  1.],\n",
       "       [ 5.,  6.,  7.,  8.,  9.,  1.],\n",
       "       [10., 11., 12., 13., 14.,  1.],\n",
       "       [15., 16., 17., 18., 19.,  1.],\n",
       "       [20., 21., 22., 23., 24.,  1.]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = np.ones(5).reshape(5,1)\n",
    "\n",
    "#añadir una nueva columna similar a las filas\n",
    "np.hstack((dataset,bias))\n",
    "\n",
    "# tambien podemos hacerlo sin remodelar la columna y unirla a nuestro ndArray\n",
    "bias = np.ones(5)\n",
    "np.column_stack((dataset,bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  1,  3,  4],\n",
       "       [ 5,  6,  7,  1,  8,  9],\n",
       "       [10, 11, 12,  1, 13, 14],\n",
       "       [15, 16, 17,  1, 18, 19],\n",
       "       [20, 21, 22,  1, 23, 24]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inserecion en una posición específica\n",
    "np.insert(dataset, 3, bias, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sparse arrays(matrices dispersas)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37964de45fa06bc99b754cfc54b483ffac9c133df6b16baa187ef33bd89a6318"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
